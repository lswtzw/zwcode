#unet网络模型的darknet框架实现
[net]
batch=8
subdivisions=1
width=128
height=128
channels=3
#上面的代码和显存相关,可以用nvidia-smi查看显存占用,不足的话训练不动并报cuda错

#下面的代码和学习速度有关
momentum=0.9
decay=0.0001
learning_rate=0.0001
max_batches = 8000
policy=steps
steps=6400,7200
scales=0.1,0.1

#1代表启用,0代表不启用,默认是0,实测在此处的效果的确并不好
adam=0 						
 
#默认的内存(不是显存)限制是1024MB
workspace_size_limit_MB=2048 


#上面是头部#
###########################################
###########################################
###########################################
#下面是身体#


#第0层开始!
[conv]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=relu


[conv]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=relu


[maxpool]
size=2
stride=2


[conv]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=relu


[conv]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=relu


[maxpool]
size=2
stride=2


[conv]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=relu


[conv]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=relu


[maxpool]
size=2
stride=2


[conv]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=relu


[conv]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=relu


[dropout]
probability=.5
dropblock_size_abs=7
dropblock=0


[maxpool]
size=2
stride=2


[conv]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=relu


[conv]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=relu


[dropout]
probability=.5
dropblock_size_abs=7
dropblock=0


[upsample]
stride=2


[conv]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=relu


[route]
layers = -1, -7


[conv]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=relu


[conv]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=relu


[upsample]
stride=2


[conv]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=relu


[route]
layers = -1, 7


[conv]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=relu


[conv]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=relu


[upsample]
stride=2

[conv]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=relu


[route]
layers = -1, 4


[conv]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=relu


[conv]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=relu

[upsample]
stride=2

[conv]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=relu


[route]
layers = -1, 1

[conv]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=relu


[conv]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=relu




#上面是身体#
###########################################
###########################################
###########################################
#下面是尾部#




#下面这个是原论文的尾层,但从2张图的输出,改成了3张
[conv]
filters=3
size=1
stride=1
pad=0
batch_normalize=1
activation=logistic
#上面这个是二分类输出的激活函数类似于sigmod


#计算损失和反馈
[cost]
type=sse






