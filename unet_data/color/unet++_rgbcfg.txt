#unet++网络模型的darknet框架实现
[net]
batch=4
subdivisions=1
width=128
height=128
channels=3
#上面的代码和显存相关,可以用nvidia-smi查看显存占用,不足的话训练不动并报cuda错

#下面的代码和学习速度有关
momentum=0.9
decay=0.0001
learning_rate=0.0001
max_batches = 8000
policy=steps
steps=6400,7200
scales=0.1,0.1

#1代表启用,0代表不启用,默认是0
adam=0 						 

#默认的内存(不是显存)限制是1024MB
workspace_size_limit_MB=2048


#上面是头部#
###########################################
###########################################
###########################################
#下面是身体#


#第0层,输入1,输出64, 输出unet++的网络pos=0,0  
#pos理解:第一个是行,第二个是列
[conv]
filters=64
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第1层,输入64,输出64, 输出unet++的网络pos=0,0
[conv]
filters=64
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第2层,输入64,输出64,面积/2,列坐标+1,输出unet++的网络pos=1,0
[maxpool]
size=2
stride=2


#第3层,输入64,输出128, 输出unet++的网络pos=1,0
[conv]
filters=128
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第4层,输入128,输出128, 输出unet++的网络pos=1,0
[conv]
filters=128
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第5层,输入128,输出128,面积x2,行坐标-1,列坐标+1,输出unet++的网络pos=0,1
[upsample]
stride=2


#第6层,输入128,输出64, 输出unet++的网络pos=0,1
[conv]
filters=64
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第7层,输入64,输出128, 输出unet++的网络pos=0,1
[route]
layers = 1, -1


#第8层,输入128,输出64, 输出unet++的网络pos=0,1
[conv]
filters=64
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第9层,输入64,输出64, 输出unet++的网络pos=0,1
[conv]
filters=64
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第10层,"抠出"第4层继续处理,输出unet++的网络pos=1,0
[route]
layers = 4


#第11层,输入128,输出128,面积/2,列坐标+1,输出unet++的网络pos=2,0
[maxpool]
size=2
stride=2


#第12层,输入128,输出256, 输出unet++的网络pos=2,0
[conv]
filters=256
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第13层,输入256,输出256, 输出unet++的网络pos=2,0
[conv]
filters=256
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第14层,输入256,输出256,面积x2,行坐标-1,列坐标+1,输出unet++的网络pos=1,1
[upsample]
stride=2


#第15层,输入256,输出128, 输出unet++的网络pos=1,1
[conv]
filters=128
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第16层,与pos=1,0合并,输入128,输出256, 输出unet++的网络pos=1,1
[route]
layers = 4,-1


#第17层,输入?,输出128, 输出unet++的网络pos=1,1
[conv]
filters=128
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第18层,输入128,输出128, 输出unet++的网络pos=1,1
[conv]
filters=128
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第19层,输入128,输出128,面积x2,行坐标-1,列坐标+1,输出unet++的网络pos=0,2
[upsample]
stride=2


#第20层,输入256,输出64, 输出unet++的网络pos=0,2
[conv]
filters=64
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第21层,与pos=0,1合并,与pos=0,0合并,输入64,输出?, 输出unet++的网络pos=0,2
[route]
layers = 1,9,-1


#第22层,输入?,输出64, 输出unet++的网络pos=0,2
[conv]
filters=64
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第23层,输入64,输出64, 输出unet++的网络pos=0,2
[conv]
filters=64
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第24层,"抠出"第13层继续处理,输出unet++的网络pos=2,0
[route]
layers = 13


#第25层,输入256,输出256,面积/2,行坐标+1,输出unet++的网络pos=3,0
[maxpool]
size=2
stride=2


#第26层,输入256,输出512, 输出unet++的网络pos=3,0
[conv]
filters=512
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第27层,输入512,输出512, 输出unet++的网络pos=3,0
[conv]
filters=512
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第28层,输入512,输出512,面积x2,行坐标-1,列坐标+1,输出unet++的网络pos=2,1
[upsample]
stride=2


#第29层,输入512,输出256, 输出unet++的网络pos=2,1
[conv]
filters=256
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第30层,与pos=2,0合并,输入256,输出?, 输出unet++的网络pos=2,1
[route]
layers = 13,-1


#第31层,输入?,输出256, 输出unet++的网络pos=2,1
[conv]
filters=256
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第32层,输入256,输出256, 输出unet++的网络pos=2,1
[conv]
filters=256
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第33层,输入256,输出256,面积x2,行坐标-1,列坐标+1,输出unet++的网络pos=1,2
[upsample]
stride=2


#第34层,输入256,输出128, 输出unet++的网络pos=1,2
[conv]
filters=128
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第35层,与pos=1,0合并,与pos=1,1合并,输入128,输出?, 输出unet++的网络pos=1,2
[route]
layers = 4,18,-1


#第36层,输入?,输出128, 输出unet++的网络pos=1,2
[conv]
filters=128
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第37层,输入128,输出128, 输出unet++的网络pos=1,2
[conv]
filters=128
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第38层,输入128,输出128,面积x2,行坐标-1,列坐标+1,输出unet++的网络pos=0,3
[upsample]
stride=2


#第39层,输入128,输出64, 输出unet++的网络pos=0,3
[conv]
filters=64
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第40层,与pos=0,0合并,与pos=0,1合并,与pos=0,2合并,输入64,输出?, 输出unet++的网络pos=0,3
[route]
layers = 1,9,23,-1


#第41层,输入?,输出64, 输出unet++的网络pos=0,3
[conv]
filters=64
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第42层,输入64,输出64, 输出unet++的网络pos=0,3
[conv]
filters=64
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第43层,"抠出"第27层继续处理,输出unet++的网络pos=3,0
[route]
layers = 27


#第44层,输入512,输出512,面积/2,列坐标+1,输出unet++的网络pos=4,0
[maxpool]
size=2
stride=2


#第45层,输入512,输出1024, 输出unet++的网络pos=4,0
[conv]
filters=1024
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第46层,输入1024,输出1024, 输出unet++的网络pos=4,0
[conv]
filters=1024
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第47层,输入1024,输出1024,面积x2,行坐标-1,列坐标+1,输出unet++的网络pos=3,1
[upsample]
stride=2


#第48层,输入1024,输出512, 输出unet++的网络pos=3,1
[conv]
filters=512
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第49层,与pos=3,0合并,输入512,输出?, 输出unet++的网络pos=3,1
[route]
layers = 27,-1


#第50层,输入512,输出512, 输出unet++的网络pos=3,1
[conv]
filters=512
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第51层,输入512,输出512, 输出unet++的网络pos=3,1
[conv]
filters=512
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第52层,输入512,输出512,面积x2,行坐标-1,列坐标+1,输出unet++的网络pos=2,2
[upsample]
stride=2


#第53层,输入512,输出256, 输出unet++的网络pos=2,2
[conv]
filters=256
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第54层,与pos=2,0合并,与pos=2,1合并,输入256,输出?, 输出unet++的网络pos=2,2
[route]
layers = 13,32,-1


#第55层,输入512,输出256, 输出unet++的网络pos=2,2
[conv]
filters=256
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第56层,输入512,输出256, 输出unet++的网络pos=2,2
[conv]
filters=256
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第57层,输入256,输出256,面积x2,行坐标-1,列坐标+1,输出unet++的网络pos=1,3
[upsample]
stride=2


#第58层,输入512,输出256, 输出unet++的网络pos=1,3
[conv]
filters=128
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第59层,与pos=1,0合并,与pos=1,1合并,与pos=1,2合并,输入128,输出?, 输出unet++的网络pos=1,3
[route]
layers = 4,18,37,-1


#第60层,输入?,输出128, 输出unet++的网络pos=1,3
[conv]
filters=128
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第61层,输入?,输出128, 输出unet++的网络pos=1,3
[conv]
filters=128
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第62层,输入128,输出128,面积x2,行坐标-1,列坐标+1,输出unet++的网络pos=0,4
[upsample]
stride=2


#第63层,输入128,输出64, 输出unet++的网络pos=0,4
[conv]
filters=64
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第64层,与pos=0,0合并,与pos=0,1合并,与pos=0,2合并,与pos=0,3合并,输入64,输出?, 输出unet++的网络pos=0,4
[route]
layers = 1,9,23,42,-1


#第65层,输入?,输出64, 输出unet++的网络pos=0,4
[conv]
filters=64
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第66层(按常规开发模式补充的步骤)网络pos=0,4
[dropout]
dropblock=1
dropblock_size_abs=7
probability=.2


#第67层,输入64,输出64, 输出unet++的网络pos=0,4
[conv]
filters=64
size=3
stride=1
pad=1
activation=relu
batch_normalize=1


#第68层(按常规开发模式补充的步骤)网络pos=0,4
[dropout]
dropblock=1
dropblock_size_abs=7
probability=.2




#上面是身体#
###########################################
###########################################
###########################################
#下面是尾部#




#下面是原论文的--->尾层 (输出3张Filter图,代表3个通道RGB)
[conv]
filters=3
size=1
stride=1
pad=0
batch_normalize=1
activation=logistic


#计算损失和反馈
[cost]
type=sse





